{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk, os\n",
    "from utils.morpho_tagger import MorphoTagger\n",
    "from gensim.models.fasttext import FastText\n",
    "from functools import reduce\n",
    "import argparse\n",
    "from wordcloud import WordCloud\n",
    "from time import time\n",
    "from gensim.similarities import WmdSimilarity\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from datetime import datetime\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class FastTextConfig:\n",
    "    def __init__(self, embedding_size, window_size, min_word, down_sampling):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "        self.min_word = min_word\n",
    "        self.down_sampling = down_sampling\n",
    "        \n",
    "class FastTextSimilarityModel:\n",
    "    def __init__(self, file_path, conf):\n",
    "        def _load(file, l):\n",
    "            with open(file_path+file, \"r\", encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    line = line[:-1]\n",
    "                    # just different sentences\n",
    "                    if line not in l:\n",
    "                        l.append(line)\n",
    "        \n",
    "        self.sentences_pos = []\n",
    "        self.sentences_pos_processed = []\n",
    "        self.sentences_neg = []\n",
    "        self.sentences_neg_processed = []\n",
    "        self.model_conf = conf\n",
    "        self.model_pos = None\n",
    "        self.model_neg = None\n",
    "        \n",
    "        \n",
    "        _load('dataset_positive.txt', self.sentences_pos)\n",
    "        _load('dataset_negative.txt', self.sentences_neg)\n",
    "        \n",
    "        self.sentences_pos_len = len(self.sentences_pos)\n",
    "        self.sentences_neg_len = len(self.sentences_neg)\n",
    "        \n",
    "        self.sentences_pos_len = 500\n",
    "        self.sentences_pos = self.sentences_pos[:self.sentences_pos_len]\n",
    "       \n",
    "\n",
    "    def preprocess(self, tagger: MorphoTagger, sentences):\n",
    "        sentences_processed = []\n",
    "\n",
    "\n",
    "        for sentence in sentences:\n",
    "            s = []\n",
    "            l = reduce(lambda x, y: x + y, tagger.pos_tagging(sentence, False))\n",
    "            for idx, wp in enumerate(l):\n",
    "                s.append(wp.lemma)\n",
    "            sentences_processed.append(s)\n",
    "            \n",
    "        \n",
    "        return sentences_processed\n",
    "\n",
    "    def train_similarity(self, sentences_processed):\n",
    "        model = FastText(sentences_processed,\n",
    "                        size=self.model_conf.embedding_size,\n",
    "                        window=self.model_conf.window_size,\n",
    "                        min_count=self.model_conf.min_word,\n",
    "                        sample=self.model_conf.down_sampling,\n",
    "                        sg=1,\n",
    "                        iter=100)\n",
    "        model.init_sims(replace=True)\n",
    "        return model\n",
    "    \n",
    "    def set_pos(self, sentences):\n",
    "        self.sentences_pos_processed = sentences\n",
    "    \n",
    "    def set_neg(self, sentences):\n",
    "        self.sentences_neg_processed = sentences\n",
    "    \n",
    "    def set_pos_model(self, model):\n",
    "        self.model_pos = model\n",
    "    \n",
    "    def set_neg_model(self, model):\n",
    "        self.model_neg = model\n",
    "        \n",
    "    def word_cloud(self, sentences, category, className):\n",
    "        tokens = [token for sentence in sentences for token in sentence]\n",
    "        text = ' '.join(tokens)\n",
    "        wordcloud = WordCloud(max_font_size=40, width=600, \n",
    "                              height=400, background_color='white', \n",
    "                              max_words=200, relative_scaling=1.0).generate_from_text(text)\n",
    "\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        wordcloud.to_file('./tmp/' + category + '-' +className + '.jpg')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.49 GiB for an array with shape (2000000, 200) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-4ebc77c868a1>\u001b[0m in \u001b[0;36mtrain_similarity\u001b[0;34m(self, sentences_processed)\u001b[0m\n\u001b[1;32m     72\u001b[0m                         \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_sampling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                         \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                         iter=100)\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, sg, hs, size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, compatible_hash)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m             self.train(\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \"\"\"\n\u001b[1;32m    711\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_ngrams_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36minit_ngrams_weights\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m   2217\u001b[0m         \u001b[0;31m#    time because the vocab is not initialized at that stage.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2219\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_ngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrand_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrams_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_ngrams_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_vocab_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 1.49 GiB for an array with shape (2000000, 200) and data type float32"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tagger = MorphoTagger()\n",
    "tagger.load_tagger(\"external/morphodita/czech-morfflex-pdt-161115-no_dia-pos_only.tagger\")\n",
    "\n",
    "conf = FastTextConfig(100, 10, 5, 1e-2)\n",
    "\n",
    "fastTextModel = FastTextSimilarityModel(\"./\", conf)\n",
    "fastTextModel.set_pos(fastTextModel.preprocess(tagger, fastTextModel.sentences_pos))\n",
    "#fastTextModel.set_neg(fastTextModel.preprocess(tagger, fastTextModel.sentences_neg))\n",
    "\n",
    "fastTextModel.set_pos_model(fastTextModel.train_similarity(fastTextModel.sentences_pos_processed))\n",
    "#fastTextModel.set_neg_model(fastTextModel.train_similarity(fastTextModel.sentences_neg_processed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(fastTextModel.sentences_pos_len)\n",
    "#print(fastTextModel.sentences_neg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantically_similar_words = {words: [item[0] for item in fastTextModel.model_pos.wv.most_similar([words], topn=5)]\n",
    "                                      for words in\n",
    "                                      ['cena', 'kabel', \n",
    "                                       'manipulace', 'hadice', \n",
    "                                       'nádoba', 'kabel', \n",
    "                                       'filtr','šnůra',\n",
    "                                      'kvalita']}\n",
    "\n",
    "for k, v in semantically_similar_words.items():\n",
    "    print(k + \":\" + str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fastTextModel.model_pos.wv.similarity(w1='doba', w2='nádoba'))\n",
    "print(fastTextModel.model_pos.wv.similarity(w1='kabel', w2='hadice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "all_similar_words = sum([[k] + v for k, v in semantically_similar_words.items()], [])\n",
    "\n",
    "print(all_similar_words)\n",
    "print(type(all_similar_words))\n",
    "print(len(all_similar_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = fastTextModel.model_pos.wv[all_similar_words]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "p_comps = pca.fit_transform(word_vectors)\n",
    "word_names = all_similar_words\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.scatter(p_comps[:, 0], p_comps[:, 1], c='red')\n",
    "\n",
    "for word_names, x, y in zip(word_names, p_comps[:, 0], p_comps[:, 1]):\n",
    "    plt.annotate(word_names, xy=(x+0.06, y+0.03), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = fastTextModel.preprocess(tagger,['Snadná obsluha, sestavení i čištění po použití.'])[0]\n",
    "s2 = fastTextModel.preprocess(tagger,['Snadná obsluha, demontáž-montáž, výměna sáčku a filtru.'])[0]\n",
    "distance = fastTextModel.model_pos.wv.wmdistance(s1, s2)\n",
    "print('distance = %.4f' % distance)\n",
    "distance = fastTextModel.model_pos.wv.wmdistance(s2, s1)\n",
    "print('distance = %.4f' % (1-distance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# positive are loaded in fastext model\n",
    "fastTextModel.word_cloud(fastTextModel.sentences_pos_processed, 'vysavace' , 'positive')\n",
    "fastTextModel.word_cloud(fastTextModel.sentences_neg_processed, 'vysavace' , 'positive')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_best = fastTextModel.sentences_pos_len\n",
    "start = time()\n",
    "wmd_sim = WmdSimilarity(fastTextModel.sentences_pos_processed, fastTextModel.model_pos, num_best=num_best)\n",
    "print('It took {:02} seconds to run.'.format(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "for i in range(0,10):\n",
    "    sims = wmd_sim[s1]\n",
    "\n",
    "print('It took {:02} seconds to run.'.format(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "sims_multi = wmd_sim[fastTextModel.preprocess(tagger,['Velmi silné vysávání.'])[0]]\n",
    "print('It took {:02} seconds to run.'.format(time() - start))\n",
    "\n",
    "for i in range(num_best):\n",
    "    print('sim = {:04}'.format(sims_multi[i][1]))\n",
    "    print(fastTextModel.sentences_pos[sims_multi[i][0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = wmd_sim[fastTextModel.preprocess(tagger,['Snadná obsluha a pohodové vysávání.'])[0]]\n",
    "\n",
    "for i in range(num_best):\n",
    "    print('sim = {:04}'.format(sims[i][1]))\n",
    "    print(fastTextModel.sentences_pos[sims[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def worker(sen):\n",
    "    out = {\n",
    "        'sentence': sen,\n",
    "        'sim_list': [],\n",
    "    }\n",
    "    sim = wmd_sim[sen]\n",
    "    for index in range(fastTextModel.sentences_pos_len):\n",
    "        d = {\n",
    "            'sim': sims[index][1],\n",
    "            'sentence': fastTextModel.sentences_pos[sims[index][0]]\n",
    "        }\n",
    "        out['sim_list'].append(d)\n",
    "    return out\n",
    "\n",
    "start = time()\n",
    "out = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:        \n",
    "    future_to_sim_d = {executor.submit(worker, sentence): sentence for sentence in fastTextModel.sentences_pos[:10]}\n",
    "    for future in concurrent.futures.as_completed(future_to_sim_d):\n",
    "       sim_d= future_to_sim_d[future]\n",
    "\n",
    "\n",
    "print('It took {:02} seconds to run.'.format(time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fse.models import Average, SIF\n",
    "from fse import IndexedList\n",
    "#model = Average(fastTextModel.model_pos)\n",
    "model = SIF(fastTextModel.model_pos)\n",
    "model.train(IndexedList(fastTextModel.sentences_pos_processed))\n",
    "model.sv.similarity(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(model.sv.similarity(0,1).round(3))\n",
    "print(model.sv.distance(0,1).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = fastTextModel.model_pos.wv.wmdistance(fastTextModel.sentences_pos_processed[0],\n",
    "                                                 fastTextModel.sentences_pos_processed[1])\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentences_vectors = []\n",
    "i = 0\n",
    "for vector in model.sv:\n",
    "    print(i)\n",
    "    i += 1\n",
    "    sentences_vectors.append(vector)\n",
    "    if i == 500:\n",
    "        break\n",
    "    \n",
    "    \n",
    "s = IndexedList(fastTextModel.sentences_pos_processed)\n",
    "print(s[0])\n",
    "model.sv.most_similar(0, indexable=s.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence2vec similarities\n",
    "sim_matrix = []\n",
    "dist_matrix = []\n",
    "\n",
    "for i in range(fastTextModel.sentences_pos_len):\n",
    "    vec_sims = []\n",
    "    vec_dists = []\n",
    "    for j in range(fastTextModel.sentences_pos_len):\n",
    "        val = model.sv.similarity(i,j)\n",
    "        if val < 0.0:\n",
    "            val = 0.0\n",
    "        if val > 1.0:\n",
    "            val = 1.0\n",
    "        vec_sims.append(val)\n",
    "        vec_dists.append(1.0-val)\n",
    "    sim_matrix.append(np.array(vec_sims))\n",
    "    dist_matrix.append(np.array(vec_dists))\n",
    "\n",
    "print(len(sim_matrix))\n",
    "print(len(dist_matrix))\n",
    "print(len(sentences_vectors))\n",
    "\n",
    "dist_matrix = np.array(dist_matrix)\n",
    "sim_matrix = np.array(sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 15\n",
    "rng = random.Random(datetime.now())\n",
    "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=60,\n",
    "                                 avoid_empty_clusters=True, rng=rng)\n",
    "\n",
    "assigned_clusters = kclusterer.cluster(sim_matrix, assign_clusters=True)\n",
    "output = {}\n",
    "for k in range(0, num_clusters):\n",
    "    output[k] = []\n",
    "    \n",
    "for j, sen in enumerate(fastTextModel.sentences_pos):\n",
    "    output[assigned_clusters[j]].append(sen+'\\t'+str(j))\n",
    "    \n",
    "\n",
    "dir = \"kmeans_clusters_sent2vec_wmd_similarity_cos_\"+str(num_clusters)\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "#f = open(\"clusters\"+str(num_clusters)+\".txt\", \"w\", encoding='utf-8')\n",
    "for key, value in output.items():\n",
    "    with open(dir+\"/\"+str(key)+\".txt\",  \"w\", encoding='utf-8') as file:\n",
    "        print(\"cluster: \" +str(key) + \" sentences: \" +str(len(value)))\n",
    "        for val in value:\n",
    "            file.write(val + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# sent2vec embeddings and cosine distance\n",
    "num_clusters = 15\n",
    "rng = random.Random(datetime.now())\n",
    "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=60,\n",
    "                                 avoid_empty_clusters=True, rng=rng)\n",
    "\n",
    "labels = kclusterer.cluster(sentences_vectors, assign_clusters=True)\n",
    "\n",
    "cnt = Counter(labels)\n",
    "print(cnt)\n",
    "\n",
    "with open('kmeans_sent2vec_cos_'+str(num_clusters)+'.tsv', 'w') as file:\n",
    "    for j, sen in enumerate(fastTextModel.sentences_pos):\n",
    "        file.write(sen + '\\t' + str(labels[j]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 15\n",
    "rng = random.Random(datetime.now())\n",
    "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=60,\n",
    "                                 avoid_empty_clusters=True, rng=rng)\n",
    "\n",
    "labels = kclusterer.cluster(dist_matrix, assign_clusters=True)\n",
    "cnt = Counter(labels)\n",
    "print(cnt)\n",
    "\n",
    "with open('kmeans_wmd_distance_cos'+str(num_clusters)+'.tsv', 'w') as file:\n",
    "    for j, sen in enumerate(fastTextModel.sentences_pos):\n",
    "        file.write(sen + '\\t' + str(labels[j]) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 15\n",
    "rng = random.Random(datetime.now())\n",
    "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=60,\n",
    "                                 avoid_empty_clusters=True, rng=rng)\n",
    "\n",
    "labels = kclusterer.cluster(sim_matrix, assign_clusters=True)\n",
    "cnt = Counter(labels)\n",
    "print(cnt)\n",
    "\n",
    "with open('kmeans_wmd_similarity_cos'+str(num_clusters)+'.tsv', 'w') as file:\n",
    "    for j, sen in enumerate(fastTextModel.sentences_pos):\n",
    "        file.write(sen + '\\t' + str(labels[j]) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 15\n",
    "dir = \"kmeans_wmd_similarity_cos_\"+str(num_clusters)\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    tmp_sim_matrix = []\n",
    "    cluster_sentences = []\n",
    "    with open(dir+\"/\"+str(2)+\".txt\",  \"r\", encoding='utf-8') as file:\n",
    "        cluster_sentences = [int(line[:-1].split('\\t')[1]) for line in file]\n",
    "        \n",
    "    \n",
    "    for i in cluster_sentences:\n",
    "        vec_sims = []\n",
    "        for j in cluster_sentences:\n",
    "            vec_sims.append(model.sv.similarity(i,j))\n",
    "        tmp_sim_matrix.append(vec_sims)\n",
    "    print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in tmp_sim_matrix]))\n",
    "    break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "clustering = AffinityPropagation(damping=0.7, affinity='precomputed', convergence_iter=20).fit(sim_matrix)\n",
    "\n",
    "labels = clustering.labels_\n",
    "\n",
    "no_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(no_clusters)\n",
    "\n",
    "with open('AffinityPropagation_wmd_sim.tsv', 'w') as file:\n",
    "    for j, sen in enumerate(fastTextModel.sentences_pos):\n",
    "        file.write(sen + '\\t' + str(labels[j]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(eps=0.3, metric='precomputed', min_samples=10, algorithm='brute').fit(dist_matrix)\n",
    "\n",
    "\n",
    "labels = clustering.labels_\n",
    "no_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(no_clusters)\n",
    "\n",
    "with open('DBSCAN_wmd_sim.tsv', 'w') as file:\n",
    "    for j, sen in enumerate(fastTextModel.sentences_pos):\n",
    "        file.write(sen + '\\t' + str(labels[j]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clustering = hdbscan.HDBSCAN(min_cluster_size=10, metric='precomputed', min_samples=10).fit(dist_matrix)\n",
    "\n",
    "labels = clustering.labels_\n",
    "\n",
    "no_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(no_clusters)\n",
    "\n",
    "\n",
    "with open('HDBSCAN_wmd_dist.tsv', 'w') as file:\n",
    "    for j, sen in enumerate(fastTextModel.sentences_pos):\n",
    "        file.write(sen + '\\t' + str(labels[j]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clustering = AgglomerativeClustering(affinity='precomputed', linkage='ward', n_clusters=5)\n",
    "\n",
    "labels = clustering.fit_predict(dist_matrix)\n",
    "\n",
    "no_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(no_clusters)\n",
    "\n",
    "with open('AgglomerativeClustering_wmd_sim.tsv', 'w') as file:\n",
    "    for j, sen in enumerate(fastTextModel.sentences_pos):\n",
    "        file.write(sen + '\\t' + str(labels[j]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}